# nlp_processor.py
import re
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class VietnameseNLProcessor:
    def __init__(self, config, db_manager):
        self.config = config
        self.db = db_manager
        self.setup_advanced_patterns()
    
    def setup_advanced_patterns(self):
        """Thi·∫øt l·∫≠p patterns n√¢ng cao cho ti·∫øng Vi·ªát t·ª± nhi√™n"""
        self.intent_patterns = {
            'schedule': [
                # C√°c c√°ch di·ªÖn ƒë·∫°t t·∫°o l·ªãch th√¥ng th∆∞·ªùng
                r'.*(ƒë·∫∑t|l·∫≠p|t·∫°o|th√™m|ghi|th√™m v√†o|ƒëƒÉng k√Ω|ƒëƒÉng k√≠)\s+(l·ªãch|cu·ªôc h·ªçp|s·ª± ki·ªán|b√°o th·ª©c|nh·∫Øc nh·ªü|h·∫πn|b√°o|c√¥ng vi·ªác|vi·ªác|c·∫ßn l√†m).*',
                r'.*b√°o th·ª©c\s+.*',
                r'.*nh·∫Øc nh·ªü\s+.*',
                r'.*nh·∫Øc t√¥i.*',
                r'.*ƒë√°nh th·ª©c.*',
                r'.*h·∫πn\s+.*',
                r'.*(ph·∫£i|nh·ªõ|c·∫ßn)\s+(d·∫≠y|t·ªânh|ƒë√°nh th·ª©c|u·ªëng thu·ªëc|l√†m|th·ª±c hi·ªán|ho√†n th√†nh).*',
                r'.*cho t√¥i.*b√°o th·ª©c.*',
                r'.*t·∫°o cho t√¥i.*l·ªãch.*',
                r'.*c√≥ vi·ªác.*',
                r'.*c·∫ßn l√†m.*',
                r'.*ph·∫£i l√†m.*',
                r'.*c√≥ h·∫πn.*',
                r'.*l√™n l·ªãch.*',
                r'.*s·∫Øp x·∫øp.*',
            ],
            'query': [
                # C√°c c√°ch di·ªÖn ƒë·∫°t xem l·ªãch
                r'.*(xem|ki·ªÉm tra|tra c·ª©u|hi·ªÉn th·ªã|li·ªát k√™|xem th·ª≠|cho xem|cho t√¥i xem|hi·ªán|hi·ªÉn th·ªã|li·ªát k√™|k·ªÉ|n√≥i)\s+(l·ªãch|l·ªãch tr√¨nh|c√¥ng vi·ªác|vi·ªác|s·ª± ki·ªán|h·∫πn|b√°o th·ª©c).*',
                r'.*(c√≥|l·ªãch)\s+g√¨\s+.*',
                r'.*xem l·ªãch.*',
                r'.*l·ªãch tr√¨nh.*',
                r'.*t·∫•t c·∫£ l·ªãch tr√¨nh.*',
                r'.*l·ªãch tr√¨nh hi·ªán c√≥.*',
                r'.*l·ªãch\s+(ng√†y mai|h√¥m nay|tu·∫ßn n√†y|th√°ng n√†y|nƒÉm nay).*',
                r'.*h√¥m nay c√≥ g√¨.*',
                r'.*ng√†y mai c√≥ g√¨.*',
                r'.*tu·∫ßn n√†y c√≥ g√¨.*',
                r'.*c√°c vi·ªác c·∫ßn l√†m.*',
                r'.*c√¥ng vi·ªác s·∫Øp t·ªõi.*',
                r'.*s·ª± ki·ªán s·∫Øp t·ªõi.*',
                r'.*h·∫πn s·∫Øp t·ªõi.*',
                r'.*l·ªãch c·ªßa t√¥i.*',
                r'.*k·∫ø ho·∫°ch.*',
                r'.*c√°c l·ªãch tr√¨nh hi·ªán c√≥.*',
                r'.*t·∫•t c·∫£ c√°c l·ªãch tr√¨nh.*',
                r'.*danh s√°ch l·ªãch tr√¨nh.*',
            ],
            'update': [
                # C√°c c√°ch di·ªÖn ƒë·∫°t s·ª≠a l·ªãch
                r'.*(s·ª≠a|thay ƒë·ªïi|ch·ªânh s·ª≠a|c·∫≠p nh·∫≠t|ƒë·ªïi|s·ª≠a l·∫°i|ch·ªânh|thay|ƒëi·ªÅu ch·ªânh|update)\s+(l·ªãch|l·ªãch tr√¨nh|ti√™u ƒë·ªÅ|t√™n|th√¥ng tin).*',
                r'.*(s·ª≠a|thay ƒë·ªïi|ch·ªânh s·ª≠a)\s+(ti√™u ƒë·ªÅ|t√™n).*',
                r'.*ƒë·ªïi t√™n l·ªãch.*',
                r'.*c·∫≠p nh·∫≠t l·ªãch.*',
                r'.*s·ª≠a\s+.*th√†nh\s+.*',
                r'.*ƒë·ªïi\s+.*th√†nh\s+.*',
                r'.*thay ƒë·ªïi\s+.*th√†nh\s+.*',
                r'.*s·ª≠a l·∫°i\s+.*th√†nh\s+.*',
                r'.*cho t√¥i s·ª≠a.*',
                r'.*ch·ªânh s·ª≠a.*',
                r'.*c·∫≠p nh·∫≠t.*',
                r'.*thay ƒë·ªïi th√¥ng tin.*',
                r'.*ƒë·ªïi t√™n.*',
            ],
            'delete': [
                # C√°c c√°ch di·ªÖn ƒë·∫°t x√≥a l·ªãch
                r'.*(x√≥a|xo√°|h·ªßy|h·ªßy b·ªè|x√≥a b·ªè|d·ª´ng|ng·ª´ng|x√≥a ƒëi|h·ªßy ƒëi|g·ª°|remove|delete)\s+(l·ªãch|l·ªãch tr√¨nh|b√°o th·ª©c|s·ª± ki·ªán|h·∫πn|c√¥ng vi·ªác|vi·ªác).*',
                r'.*x√≥a l·ªãch.*',
                r'.*h·ªßy l·ªãch.*',
                r'.*x√≥a b√°o th·ª©c.*',
                r'.*h·ªßy b√°o th·ª©c.*',
                r'.*d·ª´ng b√°o th·ª©c.*',
                r'.*x√≥a\s+.*l√∫c\s+.*',
                r'.*h·ªßy\s+.*l√∫c\s+.*',
                r'.*x√≥a\s+l·ªãch\s+tr√¨nh\s+(c√≥\s+t√™n|t√™n\s+l√†|v·ªõi\s+t√™n)\s+.*',
                r'.*h·ªßy\s+l·ªãch\s+tr√¨nh\s+(c√≥\s+t√™n|t√™n\s+l√†|v·ªõi\s+t√™n)\s+.*',
                r'.*x√≥a\s+(c√°i|cu·ªôc|v·ª•)\s+(h·ªçp|h·∫πn).*',
                r'.*h·ªßy\s+(c√°i|cu·ªôc|v·ª•)\s+(h·ªçp|h·∫πn).*',
                r'.*cho t√¥i x√≥a.*',
                r'.*gi√∫p t√¥i x√≥a.*',
                r'.*h·ªßy b·ªè.*',
                r'.*x√≥a ƒëi.*',
                r'.*g·ª° l·ªãch.*',
            ],
            'greeting': [
                r'^(xin ch√†o|ch√†o|hello|hi|ch√†o b·∫°n|ch√†o bot|ch√†o em|ch√†o anh|ch·ªã|em|anh|b·∫°n|hey|hi there).*',
            ],
            'thanks': [
                r'^(c·∫£m ∆°n|thank you|thanks|c√°m ∆°n|∆°n|ƒëa t·∫°|c·∫£m ∆°n b·∫°n|c·∫£m ∆°n nhi·ªÅu).*',
            ],
            'help': [
                r'^(help|gi√∫p|h·ªó tr·ª£|l√†m g√¨|t√≠nh nƒÉng|h∆∞·ªõng d·∫´n|ch·ª©c nƒÉng|tr·ª£ gi√∫p|support).*',
            ],
            'time_query': [
                r'.*(m·∫•y gi·ªù|m·∫•y h|bao nhi√™u gi·ªù|th·ªùi gian|gi·ªù).*',
                r'.*b√¢y gi·ªù l√† m·∫•y.*',
                r'.*gi·ªù l√† m·∫•y.*',
                r'.*cho bi·∫øt gi·ªù.*',
                r'.*m·∫•y gi·ªù r·ªìi.*',
                r'.*th·ªùi gian hi·ªán t·∫°i.*',
            ],
            'date_query': [
                r'.*h√¥m nay l√† ng√†y m·∫•y.*',
                r'.*ng√†y bao nhi√™u.*',
                r'.*th·ª© m·∫•y.*',
                r'.*cho bi·∫øt ng√†y.*',
                r'.*h√¥m nay th·ª© m·∫•y.*',
                r'.*ng√†y th√°ng.*',
            ],
        }
    
    def detect_intent(self, text: str) -> Dict[str, Any]:
        """Ph√°t hi·ªán intent v·ªõi x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n"""
        text_lower = text.lower().strip()
        logger.info(f"üîç Analyzing natural language: '{text}'")
        
        # B∆∞·ªõc 1: L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n
        cleaned_text = self._clean_and_normalize_text(text_lower)
        
        # B∆∞·ªõc 2: Ph√°t hi·ªán intent v·ªõi ƒë·ªô ∆∞u ti√™n
        intent_result = self._detect_intent_with_priority(cleaned_text)
        
        # B∆∞·ªõc 3: Ph√¢n t√≠ch chi ti·∫øt d·ª±a tr√™n intent
        enhanced_result = self._enhance_with_detailed_analysis(cleaned_text, intent_result)
        
        logger.info(f"üéØ Final analysis: {enhanced_result}")
        return enhanced_result
    
    def _clean_and_normalize_text(self, text: str) -> str:
        """L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát"""
        # Lo·∫°i b·ªè c√°c t·ª´ c·∫£m th√°n, t·ª´ d∆∞ th·ª´a
        filler_words = ['·∫°', 'nh√©', 'nha', 'ƒëi', 'n√†o', '∆°i', '√†', '·ª´m', 'nh√°', 'n√®', 'ƒë·∫•y', 'ƒë√≥']
        for word in filler_words:
            text = re.sub(r'\s+' + word + r'\s*', ' ', text)
            text = re.sub(r'^\s*' + word + r'\s+', '', text)
            text = re.sub(r'\s+' + word + r'$', '', text)
        
        # Chu·∫©n h√≥a c√°ch vi·∫øt
        replacements = {
            'xo√°': 'x√≥a',
            'bth·ª©c': 'b√°o th·ª©c',
            'nh·∫Øc nh·ªü': 'nh·∫Øc',
            'l·ªãch tr√¨nh': 'l·ªãch',
            'cu·ªôc h·ªçp': 'h·ªçp',
            's·ª± ki·ªán': 's·ª± ki·ªán',
            'c√¥ng vi·ªác': 'vi·ªác',
            'k·∫ø ho·∫°ch': 'k·∫ø ho·∫°ch'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text.strip()
    
    def _detect_intent_with_priority(self, text: str) -> Dict[str, Any]:
        """Ph√°t hi·ªán intent v·ªõi ƒë·ªô ∆∞u ti√™n v√† ƒëi·ªÉm s·ªë"""
        
        # T√≠nh ƒëi·ªÉm cho t·ª´ng intent
        intent_scores = {}
        
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, text):
                    score += 1
            
            # Th√™m ƒëi·ªÉm cho t·ª´ kh√≥a ƒë·∫∑c bi·ªát
            keyword_bonus = {
                'schedule': ['ƒë·∫∑t', 't·∫°o', 'b√°o th·ª©c', 'nh·∫Øc', 'h·∫πn', 'vi·ªác', 'l√†m'],
                'update': ['s·ª≠a', 'ƒë·ªïi', 'th√†nh', 'ch·ªânh', 'c·∫≠p nh·∫≠t'],
                'delete': ['x√≥a', 'h·ªßy', 'd·ª´ng', 'g·ª°'],
                'query': ['xem', 'ki·ªÉm tra', 'c√≥ g√¨', 'l·ªãch tr√¨nh', 't·∫•t c·∫£', 'c√°c']
            }
            
            if intent in keyword_bonus:
                for keyword in keyword_bonus[intent]:
                    if keyword in text:
                        score += 0.5
            
            if score > 0:
                intent_scores[intent] = score
        
        # Ch·ªçn intent c√≥ ƒëi·ªÉm cao nh·∫•t
        if intent_scores:
            best_intent = max(intent_scores, key=intent_scores.get)
            confidence = min(0.95, 0.7 + (intent_scores[best_intent] * 0.1))
            
            return {
                'intent': best_intent,
                'confidence': confidence,
                'method': 'pattern_scoring'
            }
        
        # Fallback: s·ª≠ d·ª•ng ph√¢n t√≠ch c∆° b·∫£n
        return self._fallback_analysis(text)
    
    def _enhance_with_detailed_analysis(self, text: str, intent_result: Dict) -> Dict[str, Any]:
        """Ph√¢n t√≠ch chi ti·∫øt d·ª±a tr√™n intent"""
        intent = intent_result['intent']
        
        if intent == 'schedule':
            analysis = self._natural_schedule_analysis(text)
        elif intent == 'query':
            analysis = self._natural_query_analysis(text)
        elif intent == 'update':
            analysis = self._natural_update_analysis(text)
        elif intent == 'delete':
            analysis = self._natural_delete_analysis(text)
        else:
            analysis = {}
        
        return {**intent_result, **analysis}
    
    def _natural_schedule_analysis(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch t·ª± nhi√™n cho t·∫°o l·ªãch"""
        # X√°c ƒë·ªãnh lo·∫°i s·ª± ki·ªán
        event_type = self._determine_event_type(text)
        
        # Extract th√¥ng tin th·ªùi gian
        time_info = self._advanced_time_extraction(text)
        
        # Extract ti√™u ƒë·ªÅ t·ª± nhi√™n
        title = self._extract_natural_title_improved(text, event_type)
        
        # Extract m√¥ t·∫£
        description = self._extract_contextual_description(text)
        
        return {
            'title': title,
            'description': description,
            'datetime': time_info['datetime'],
            'duration_minutes': 15 if event_type == 'alarm' else 60,
            'priority': 'high' if event_type == 'alarm' else 'medium',
            'category': event_type,
            'is_alarm': event_type == 'alarm',
            'time_info': time_info
        }
    
    def _extract_natural_title_improved(self, text: str, event_type: str) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ t·ª± nhi√™n - PHI√äN B·∫¢N C·∫¢I THI·ªÜN"""
        logger.info(f"üîç Extracting title from: '{text}'")
        
        # ∆Øu ti√™n 1: T√¨m ti√™u ƒë·ªÅ trong d·∫•u ngo·∫∑c k√©p
        quoted_title = self._extract_quoted_title(text)
        if quoted_title:
            logger.info(f"üìù Found quoted title: '{quoted_title}'")
            return quoted_title
        
        # ∆Øu ti√™n 2: T√¨m ti√™u ƒë·ªÅ sau t·ª´ kh√≥a "v·ªõi ti√™u ƒë·ªÅ l√†", "t√™n l√†", etc.
        keyword_title = self._extract_title_after_keywords(text)
        if keyword_title:
            logger.info(f"üìù Found keyword title: '{keyword_title}'")
            return keyword_title
        
        # ∆Øu ti√™n 3: T√¨m ti√™u ƒë·ªÅ cu·ªëi c√¢u (sau th·ªùi gian)
        end_title = self._extract_title_from_end(text)
        if end_title:
            logger.info(f"üìù Found end title: '{end_title}'")
            return end_title
        
        # ∆Øu ti√™n 4: T√¨m ti√™u ƒë·ªÅ t·ª´ n·ªôi dung ch√≠nh
        main_title = self._extract_main_content_title(text)
        if main_title:
            logger.info(f"üìù Found main title: '{main_title}'")
            return main_title
        
        # Fallback: D·ª±a v√†o lo·∫°i s·ª± ki·ªán
        if event_type == 'alarm':
            if any(word in text for word in ['d·∫≠y', 't·ªânh']):
                return "B√°o th·ª©c d·∫≠y"
            elif any(word in text for word in ['u·ªëng thu·ªëc']):
                return "B√°o th·ª©c u·ªëng thu·ªëc"
            else:
                return "B√°o th·ª©c"
        
        logger.info("üìù Using default title: 'S·ª± ki·ªán m·ªõi'")
        return "S·ª± ki·ªán m·ªõi"
    
    def _extract_main_content_title(self, text: str) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ t·ª´ n·ªôi dung ch√≠nh c·ªßa c√¢u"""
        # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
        patterns_to_remove = [
            r'.*(ƒë·∫∑t|l·∫≠p|t·∫°o|th√™m|ghi)\s+(l·ªãch|b√°o th·ª©c|nh·∫Øc)\s+(cho\s+)?(t√¥i|m√¨nh|m√¨nh|tao|t·ªõ)?\s*',
            r'.*(v√†o|l√∫c|ng√†y|v√†o ng√†y|v√†o l√∫c|h√¥m|mai|ng√†y mai)\s+.*',
            r'.*\d{1,2}(h|:\d{2})?\s*(s√°ng|chi·ªÅu|t·ªëi)?\s*',
            r'^(xin\s+)?(ch√†o|hello|hi)\s+.*',
        ]
        
        clean_text = text
        for pattern in patterns_to_remove:
            clean_text = re.sub(pattern, '', clean_text).strip()
        
        # L·∫•y c√°c t·ª´ quan tr·ªçng
        words = clean_text.split()
        important_words = []
        
        for word in words:
            if len(word) > 2 and word not in ['l√†', 'c√≥', 'v·ªõi', 'cho', 'c·ªßa', 't·ª´']:
                important_words.append(word)
        
        if important_words:
            title = ' '.join(important_words[:5])  # Gi·ªõi h·∫°n 5 t·ª´
            if len(title) > 3:
                return title
        
        return ""
    
    def _extract_quoted_title(self, text: str) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ trong d·∫•u ngo·∫∑c k√©p"""
        # T√¨m text trong d·∫•u ngo·∫∑c k√©p
        quoted_matches = re.findall(r'[""]([^""]+)[""]', text)
        if quoted_matches:
            # L·∫•y ph·∫ßn trong ngo·∫∑c k√©p cu·ªëi c√πng (th∆∞·ªùng l√† ti√™u ƒë·ªÅ)
            return quoted_matches[-1].strip()
        
        # T√¨m text trong d·∫•u nh√°y ƒë∆°n
        single_quoted_matches = re.findall(r"'([^']+)'", text)
        if single_quoted_matches:
            return single_quoted_matches[-1].strip()
        
        return ""
    
    def _extract_title_after_keywords(self, text: str) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ sau c√°c t·ª´ kh√≥a ch·ªâ ƒë·ªãnh"""
        keyword_patterns = [
            r'v·ªõi\s+ti√™u\s+ƒë·ªÅ\s+l√†\s+[""]([^""]+)[""]',
            r'ti√™u\s+ƒë·ªÅ\s+l√†\s+[""]([^""]+)[""]', 
            r't√™n\s+l√†\s+[""]([^""]+)[""]',
            r'v·ªõi\s+t√™n\s+l√†\s+[""]([^""]+)[""]',
            r'ƒë·∫∑t\s+t√™n\s+l√†\s+[""]([^""]+)[""]',
            r'g·ªçi\s+l√†\s+[""]([^""]+)[""]',
            # Kh√¥ng c√≥ d·∫•u ngo·∫∑c k√©p
            r'v·ªõi\s+ti√™u\s+ƒë·ªÅ\s+l√†\s+(.+)',
            r'ti√™u\s+ƒë·ªÅ\s+l√†\s+(.+)', 
            r't√™n\s+l√†\s+(.+)',
            r'v·ªõi\s+t√™n\s+l√†\s+(.+)',
            r'th√†nh\s+(.+)',
            r'l√†\s+(.+)',
        ]
        
        for pattern in keyword_patterns:
            match = re.search(pattern, text)
            if match:
                raw_title = match.group(1).strip()
                # L·ªçc b·ªè ph·∫ßn th·ªùi gian n·∫øu c√≥
                title = self._clean_extracted_title(raw_title)
                if title and len(title) > 2:  # √çt nh·∫•t 3 k√Ω t·ª±
                    return title
        
        return ""
    
    def _extract_title_from_end(self, text: str) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ t·ª´ cu·ªëi c√¢u (sau ph·∫ßn th·ªùi gian)"""
        # Pattern: [h√†nh ƒë·ªông] [th·ªùi gian] [ti√™u ƒë·ªÅ]
        end_patterns = [
            r'(?:l√∫c\s+\d{1,2}(?:h|:\d{2})?\s*(?:s√°ng|chi·ªÅu|t·ªëi)?\s*(?:ng√†y\s+mai|mai|h√¥m\s+nay)?\s*)(.+)',
            r'(?:v√†o\s+\d{1,2}(?:h|:\d{2})?\s*(?:s√°ng|chi·ªÅu|t·ªëi)?\s*(?:ng√†y\s+mai|mai|h√¥m\s+nay)?\s*)(.+)',
            r'(?:ng√†y\s+mai\s+l√∫c\s+\d{1,2}(?:h|:\d{2})?\s*(?:s√°ng|chi·ªÅu|t·ªëi)?\s*)(.+)',
            r'(?:h√¥m\s+nay\s+l√∫c\s+\d{1,2}(?:h|:\d{2})?\s*(?:s√°ng|chi·ªÅu|t·ªëi)?\s*)(.+)',
            r'(?:th·ª©\s+.*\s+l√∫c\s+\d{1,2}(?:h|:\d{2})?\s*)(.+)',
        ]
        
        for pattern in end_patterns:
            match = re.search(pattern, text)
            if match:
                raw_title = match.group(1).strip()
                title = self._clean_extracted_title(raw_title)
                if title and not any(word in title.lower() for word in ['v·ªõi', 'cho', 'ƒë·ªÉ', 'l√†', 'c√≥']):
                    return title
        
        return ""
    
    def _clean_extracted_title(self, raw_title: str) -> str:
        """L√†m s·∫°ch ti√™u ƒë·ªÅ ƒë√£ tr√≠ch xu·∫•t"""
        if not raw_title:
            return ""
        
        # Lo·∫°i b·ªè c√°c t·ª´ kh√≥a kh√¥ng c·∫ßn thi·∫øt ·ªü ƒë·∫ßu
        stop_starts = ['v·ªõi ti√™u ƒë·ªÅ l√†', 'ti√™u ƒë·ªÅ l√†', 't√™n l√†', 'v·ªõi t√™n l√†', 'g·ªçi l√†', 'th√†nh', 'l√†']
        for stop in stop_starts:
            if raw_title.lower().startswith(stop):
                raw_title = raw_title[len(stop):].strip()
        
        # Lo·∫°i b·ªè c√°c t·ª´ d∆∞ th·ª´a
        stop_words = ['v·ªõi', 'cho', 'v√†o', 'l√∫c', 'ng√†y', 'mai', 'h√¥m nay', 's√°ng', 'chi·ªÅu', 't·ªëi', 'nh√©', 'nha', '·∫°']
        words = raw_title.split()
        filtered_words = [word for word in words if word.lower() not in stop_words]
        
        title = ' '.join(filtered_words).strip()
        
        # Lo·∫°i b·ªè d·∫•u c√¢u th·ª´a
        title = re.sub(r'^[,\-\s]+|[,\-\s]+$', '', title)
        
        return title if title else ""
    
    def _determine_event_type(self, text: str) -> str:
        """X√°c ƒë·ªãnh lo·∫°i s·ª± ki·ªán"""
        if any(word in text for word in ['b√°o th·ª©c', 'ƒë√°nh th·ª©c', 'd·∫≠y', 't·ªânh', 'th·ª©c']):
            return 'alarm'
        elif any(word in text for word in ['h·ªçp', 'meeting', 'cu·ªôc h·ªçp', 'h·ªôi h·ªçp']):
            return 'meeting'
        elif any(word in text for word in ['nh·∫Øc', 'nh·∫Øc nh·ªü', 'reminder']):
            return 'reminder'
        elif any(word in text for word in ['si√™u th·ªã', 'mua s·∫Øm', 'ƒÉn u·ªëng', 'cafe', 'gi·∫£i tr√≠']):
            return 'personal'
        elif any(word in text for word in ['h·ªçc', 'b√†i', 'ƒë·ªì √°n', 'd·ª± √°n', 'l√†m b√†i']):
            return 'study'
        else:
            return 'general'
    
    def _advanced_time_extraction(self, text: str) -> Dict[str, Any]:
        """Tr√≠ch xu·∫•t th·ªùi gian n√¢ng cao"""
        now = datetime.now()
        
        # X√°c ƒë·ªãnh ng√†y
        date_info = self._extract_date_info(text, now)
        
        # X√°c ƒë·ªãnh gi·ªù
        time_info = self._extract_time_info(text)
        
        # K·∫øt h·ª£p ng√†y v√† gi·ªù
        target_datetime = date_info['target_date'].replace(
            hour=time_info['hour'], 
            minute=time_info['minute'], 
            second=0, 
            microsecond=0
        )
        
        # ƒêi·ªÅu ch·ªânh n·∫øu th·ªùi gian ƒë√£ qua
        if target_datetime < now and date_info['date_type'] == 'today':
            target_datetime += timedelta(days=1)
        
        return {
            'datetime': target_datetime.strftime('%Y-%m-%d %H:%M:%S'),
            'date': target_datetime.strftime('%Y-%m-%d'),
            'time': target_datetime.strftime('%H:%M'),
            'hour': time_info['hour'],
            'minute': time_info['minute'],
            'date_type': date_info['date_type'],
            'period': time_info['period']
        }
    
    def _extract_date_info(self, text: str, now: datetime) -> Dict[str, Any]:
        """Tr√≠ch xu·∫•t th√¥ng tin ng√†y"""
        # X√°c ƒë·ªãnh ng√†y d·ª±a tr√™n t·ª´ kh√≥a
        if any(word in text for word in ['mai', 'ng√†y mai']):
            return {'target_date': now + timedelta(days=1), 'date_type': 'tomorrow'}
        elif any(word in text for word in ['h√¥m nay', 'h√¥m nay', 'b√¢y gi·ªù']):
            return {'target_date': now, 'date_type': 'today'}
        elif any(word in text for word in ['h√¥m qua']):
            return {'target_date': now - timedelta(days=1), 'date_type': 'yesterday'}
        elif 'th·ª© 2' in text or 'th·ª© hai' in text:
            days_ahead = 0 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'monday'}
        elif 'th·ª© 3' in text or 'th·ª© ba' in text:
            days_ahead = 1 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'tuesday'}
        elif 'th·ª© 4' in text or 'th·ª© t∆∞' in text:
            days_ahead = 2 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'wednesday'}
        elif 'th·ª© 5' in text or 'th·ª© nƒÉm' in text:
            days_ahead = 3 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'thursday'}
        elif 'th·ª© 6' in text or 'th·ª© s√°u' in text:
            days_ahead = 4 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'friday'}
        elif 'th·ª© 7' in text or 'th·ª© b·∫£y' in text:
            days_ahead = 5 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'saturday'}
        elif 'ch·ªß nh·∫≠t' in text:
            days_ahead = 6 - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            return {'target_date': now + timedelta(days=days_ahead), 'date_type': 'sunday'}
        else:
            return {'target_date': now, 'date_type': 'today'}
    
    def _extract_time_info(self, text: str) -> Dict[str, Any]:
        """Tr√≠ch xu·∫•t th√¥ng tin gi·ªù"""
        # Pattern chi ti·∫øt cho th·ªùi gian
        time_patterns = [
            r'(\d{1,2})h\s*(\d{1,2})?\s*(s√°ng|chi·ªÅu|t·ªëi)?',
            r'(\d{1,2}):(\d{1,2})\s*(s√°ng|chi·ªÅu|t·ªëi)?',
            r'l√∫c\s*(\d{1,2})\s*(s√°ng|chi·ªÅu|t·ªëi)?',
            r'(\d{1,2})\s*gi·ªù\s*(\d{1,2})?\s*(s√°ng|chi·ªÅu|t·ªëi)?',
            r'(\d{1,2})\s*(s√°ng|chi·ªÅu|t·ªëi)',
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, text)
            if match:
                hour = int(match.group(1))
                minute = int(match.group(2)) if match.group(2) else 0
                period = match.group(3) if match.group(3) else ''
                
                # ƒêi·ªÅu ch·ªânh gi·ªù theo bu·ªïi
                if period == 'chi·ªÅu' or period == 't·ªëi':
                    if hour < 12:
                        hour += 12
                elif period == 's√°ng' and hour == 12:
                    hour = 0
                
                return {'hour': hour, 'minute': minute, 'period': period}
        
        # M·∫∑c ƒë·ªãnh 9:00 s√°ng
        return {'hour': 9, 'minute': 0, 'period': ''}
    
    def _extract_contextual_description(self, text: str) -> str:
        """Tr√≠ch xu·∫•t m√¥ t·∫£ theo ng·ªØ c·∫£nh"""
        descriptions = []
        
        if any(word in text for word in ['quan tr·ªçng', 'kh·∫©n c·∫•p', 'g·∫•p']):
            descriptions.append("quan tr·ªçng")
        if any(word in text for word in ['u·ªëng thu·ªëc']):
            descriptions.append("u·ªëng thu·ªëc")
        if any(word in text for word in ['d·∫≠y s·ªõm']):
            descriptions.append("d·∫≠y s·ªõm")
        if any(word in text for word in ['h·ªçc b√†i', 'l√†m b√†i']):
            descriptions.append("h·ªçc t·∫≠p")
        if any(word in text for word in ['mua s·∫Øm', 'si√™u th·ªã']):
            descriptions.append("mua s·∫Øm")
        
        return ", ".join(descriptions) if descriptions else ""
    
    def _natural_query_analysis(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch truy v·∫•n t·ª± nhi√™n"""
        if any(word in text for word in ['mai', 'ng√†y mai']):
            scope = 'tomorrow'
        elif any(word in text for word in ['h√¥m nay', 'h√¥m nay']):
            scope = 'today'
        elif any(word in text for word in ['tu·∫ßn', 'tu·∫ßn n√†y']):
            scope = 'week'
        elif any(word in text for word in ['t·∫•t c·∫£', 'c√°c', 'hi·ªán c√≥', 'to√†n b·ªô']):
            scope = 'all'
        else:
            scope = 'all'
        
        return {'query_scope': scope}
    
    def _natural_update_analysis(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫≠p nh·∫≠t t·ª± nhi√™n"""
        result = {}
        
        # Pattern "s·ª≠a A th√†nh B"
        change_match = re.search(r's·ª≠a\s+(.+?)\s+th√†nh\s+(.+)', text)
        if change_match:
            result['old_title'] = self._clean_title(change_match.group(1))
            result['new_title'] = self._clean_title(change_match.group(2))
        
        # T√¨m ID trong c·∫≠p nh·∫≠t
        id_match = re.search(r'(?:l·ªãch\s*tr√¨nh\s*)?(?:c√≥\s+)?id\s*(?:b·∫±ng|l√†|=\s*)?\s*(\d+)', text)
        if id_match:
            result['schedule_id'] = int(id_match.group(1))
        
        # Th√™m th·ªùi gian n·∫øu c√≥
        time_info = self._advanced_time_extraction(text)
        if time_info['datetime']:
            result['datetime'] = time_info['datetime']
        
        return result
    
    def _natural_delete_analysis(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch x√≥a t·ª± nhi√™n"""
        result = {}
        
        # Rule 1: T√¨m ID tr·ª±c ti·∫øp
        id_match = re.search(r'(?:l·ªãch\s*tr√¨nh\s*)?(?:c√≥\s+)?id\s*(?:b·∫±ng|l√†|=\s*)?\s*(\d+)', text)
        if id_match:
            result['schedule_id'] = int(id_match.group(1))
            return result
        
        # Rule 2: Pattern "x√≥a l·ªãch tr√¨nh c√≥ t√™n X"
        name_patterns = [
            r'x√≥a\s+(?:l·ªãch\s+tr√¨nh|l·ªãch)\s+(?:c√≥\s+t√™n|t√™n\s+l√†|v·ªõi\s+t√™n)\s+(.+)',
            r'h·ªßy\s+(?:l·ªãch\s+tr√¨nh|l·ªãch)\s+(?:c√≥\s+t√™n|t√™n\s+l√†|v·ªõi\s+t√™n)\s+(.+)',
            r'x√≥a\s+(.+)',
            r'h·ªßy\s+(.+)'
        ]
        
        title_keyword = None
        for pattern in name_patterns:
            match = re.search(pattern, text)
            if match:
                raw_title = match.group(1).strip()
                title_keyword = self._clean_delete_keyword(raw_title, text)
                if title_keyword:
                    break
        
        if title_keyword:
            result['title_keyword'] = title_keyword
        
        # Rule 3: Extract th·ªùi gian ƒë·ªÉ t√¨m ki·∫øm ch√≠nh x√°c h∆°n
        time_info = self._advanced_time_extraction(text)
        if time_info['datetime']:
            result['datetime'] = time_info['datetime']
        
        # Rule 4: X√°c ƒë·ªãnh lo·∫°i s·ª± ki·ªán ƒë·ªÉ t√¨m ki·∫øm
        if any(word in text for word in ['b√°o th·ª©c', 'nh·∫Øc', 'ƒë√°nh th·ª©c']):
            result['search_category'] = 'alarm'
        elif any(word in text for word in ['h·ªçp', 'h·∫πn', 'cu·ªôc h·ªçp']):
            result['search_category'] = 'meeting'
        
        return result
    
    def _clean_delete_keyword(self, raw_keyword: str, original_text: str) -> str:
        """L√†m s·∫°ch t·ª´ kh√≥a t√¨m ki·∫øm cho x√≥a"""
        if not raw_keyword:
            return ""
        
        # Lo·∫°i b·ªè th·ªùi gian v√† t·ª´ d∆∞ th·ª´a
        time_indicators = ['l√∫c', 'v√†o', 'ng√†y', 'mai', 'h√¥m nay', 's√°ng', 'chi·ªÅu', 't·ªëi']
        stop_words = ['ƒëi', 'nh√©', 'nha', '·∫°', 'cho t√¥i', 'gi√∫p t√¥i', 'gi√πm t√¥i', 'gi√∫p', 'cho']
        
        clean_keyword = raw_keyword.strip()
        
        # Lo·∫°i b·ªè ph·∫ßn th·ªùi gian
        for indicator in time_indicators:
            if indicator in clean_keyword:
                clean_keyword = clean_keyword.split(indicator)[0].strip()
        
        # Lo·∫°i b·ªè t·ª´ d∆∞ th·ª´a
        for word in stop_words:
            clean_keyword = clean_keyword.replace(word, '').strip()
        
        # Lo·∫°i b·ªè s·ªë v√† k√Ω t·ª± th·ªùi gian
        clean_keyword = re.sub(r'\d+[h:\s]*', '', clean_keyword).strip()
        
        return clean_keyword if clean_keyword else ""
    
    def _clean_title(self, title: str) -> str:
        """L√†m s·∫°ch ti√™u ƒë·ªÅ"""
        if not title:
            return ""
        
        stop_words = ['v·ªÅ', 'cho', 'v√†o', 'l√∫c', 'ng√†y', 'v√†o l√∫c', 'v√†o ng√†y']
        clean_title = title.strip()
        
        for word in stop_words:
            if clean_title.startswith(word + ' '):
                clean_title = clean_title[len(word):].strip()
            if clean_title.endswith(' ' + word):
                clean_title = clean_title[:-len(word)].strip()
        
        return clean_title
    
    def _fallback_analysis(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch fallback"""
        # Ph√¢n t√≠ch ƒë∆°n gi·∫£n cho c√°c tr∆∞·ªùng h·ª£p c∆° b·∫£n
        if any(word in text for word in ['ch√†o', 'hello', 'hi', 'xin ch√†o']):
            return {
                'intent': 'greeting',
                'confidence': 0.8,
                'method': 'fallback'
            }
        elif any(word in text for word in ['c·∫£m ∆°n', 'thanks', 'thank you']):
            return {
                'intent': 'thanks', 
                'confidence': 0.8,
                'method': 'fallback'
            }
        elif any(word in text for word in ['gi√∫p', 'help', 'h·ªó tr·ª£']):
            return {
                'intent': 'help',
                'confidence': 0.8,
                'method': 'fallback'
            }
        else:
            return {
                'intent': 'conversation',
                'response': 'Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?',
                'confidence': 0.5,
                'method': 'fallback'
            }