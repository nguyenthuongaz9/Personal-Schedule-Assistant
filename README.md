
ğŸ—“ï¸ Personal Schedule Assistant
á»¨ng dá»¥ng quáº£n lÃ½ lá»‹ch trÃ¬nh cÃ¡ nhÃ¢n thÃ´ng minh vá»›i trá»£ lÃ½ AI, há»— trá»£ tiáº¿ng Viá»‡t vÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.

https://img.shields.io/badge/Architecture-Microservices-blue
https://img.shields.io/badge/Python-3.9+-green
https://img.shields.io/badge/Next.js-14.0+-blue
https://img.shields.io/badge/AI-Ollama-orange

âœ¨ TÃ­nh nÄƒng chÃ­nh
ğŸ¤– Trá»£ lÃ½ AI thÃ´ng minh
Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn tiáº¿ng Viá»‡t - Giao tiáº¿p tá»± nhiÃªn nhÆ° nÃ³i chuyá»‡n vá»›i ngÆ°á»i

Äa dáº¡ng intent: Táº¡o lá»‹ch, xem lá»‹ch, cáº­p nháº­t, xÃ³a lá»‹ch trÃ¬nh

TÃ­ch há»£p Ollama - Cháº¡y AI model cá»¥c bá»™, báº£o máº­t dá»¯ liá»‡u

ğŸ“… Quáº£n lÃ½ lá»‹ch trÃ¬nh
Táº¡o lá»‹ch nhanh báº±ng giá»ng nÃ³i tá»± nhiÃªn

Xem lá»‹ch theo ngÃ y/tuáº§n vá»›i giao diá»‡n trá»±c quan

PhÃ¢n loáº¡i vÃ  Æ°u tiÃªn cÃ´ng viá»‡c

Nháº¯c nhá»Ÿ thÃ´ng minh

ğŸ¨ Giao diá»‡n hiá»‡n Ä‘áº¡i
Responsive design - Hoáº¡t Ä‘á»™ng trÃªn má»i thiáº¿t bá»‹

Dark/Light mode (coming soon)

Real-time updates - Cáº­p nháº­t tá»©c thÃ¬

Vietnamese UI - Giao diá»‡n tiáº¿ng Viá»‡t thÃ¢n thiá»‡n

ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng
text
Frontend (Next.js) â†â†’ Backend (Flask) â†â†’ Database (MySQL)
                            â†“
                       AI Service (Ollama)
Tech Stack
Frontend: Next.js 14, TypeScript, Tailwind CSS, Zustand

Backend: Python Flask, MySQL Connector

AI: Ollama vá»›i cÃ¡c model Mistral/TinyLlama

Database: MySQL 8.0

Container: Docker & Docker Compose

ğŸš€ Quick Start
Prerequisites
Docker & Docker Compose

4GB RAM trá»Ÿ lÃªn

10GB disk space trá»Ÿ lÃªn

CÃ i Ä‘áº·t nhanh (Recommended)
bash
# Clone repository
git clone 
cd ai_assistant_calendar_management

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ há»‡ thá»‘ng
docker compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker compose ps

# Truy cáº­p á»©ng dá»¥ng
# Frontend: http://localhost:3000
# Backend API: http://localhost:5000
# Ollama: http://localhost:11434
CÃ i Ä‘áº·t thá»§ cÃ´ng
Backend
bash
cd server

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c venv\Scripts\activate  # Windows

# CÃ i dependencies
pip install -r requirements.txt

# Cháº¡y backend
python app.py
Frontend
bash
cd frontend

# CÃ i dependencies
npm install

# Cháº¡y development server
npm run dev
Ollama (AI Service)
bash
# CÃ i Ä‘áº·t Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull model (chá»n 1)
ollama pull mistral:7b      # CÃ¢n báº±ng tá»‘t
ollama pull tinyllama       # Nháº¹ vÃ  nhanh
ollama pull codellama:7b    # Tá»‘t cho text

# Khá»Ÿi Ä‘á»™ng Ollama
ollama serve
ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng
Giao tiáº¿p vá»›i trá»£ lÃ½ AI
Báº¡n cÃ³ thá»ƒ nÃ³i chuyá»‡n tá»± nhiÃªn báº±ng tiáº¿ng Viá»‡t:

text
ğŸ’¬ "Äáº·t lá»‹ch há»p vá»›i team ngÃ y mai lÃºc 9h sÃ¡ng"
ğŸ’¬ "Xem lá»‹ch trÃ¬nh cá»§a tÃ´i hÃ´m nay"
ğŸ’¬ "TÃ´i cÃ³ lá»‹ch gÃ¬ chiá»u nay khÃ´ng?"
ğŸ’¬ "Táº¡o lá»‹ch khÃ¡m sá»©c khá»e thá»© 6 tuáº§n nÃ y"
ğŸ’¬ "Há»§y lá»‹ch há»p chiá»u nay"
CÃ¡c tÃ­nh nÄƒng chÃ­nh
Chat Interface: TrÃ² chuyá»‡n trá»±c tiáº¿p vá»›i AI assistant

Calendar View: Xem lá»‹ch trÃ¬nh dáº¡ng lá»‹ch thÃ¡ng

List View: Danh sÃ¡ch lá»‹ch trÃ¬nh chi tiáº¿t

Smart Scheduling: AI tá»± Ä‘á»™ng phÃ¢n tÃ­ch vÃ  táº¡o lá»‹ch

VÃ­ dá»¥ sá»­ dá»¥ng
python
# Táº¡o lá»‹ch trÃ¬nh qua API
import requests

response = requests.post('http://localhost:5000/api/chat', {
    'user_id': 1,
    'message': 'Äáº·t lá»‹ch há»p review dá»± Ã¡n ngÃ y mai lÃºc 14:00'
})

print(response.json())
# {
#   "success": true,
#   "message": "âœ… ÄÃ£ táº¡o lá»‹ch 'há»p review dá»± Ã¡n' vÃ o lÃºc 14:00 02/12/2024",
#   "type": "schedule_created"
# }
ğŸ”§ Cáº¥u hÃ¬nh
Environment Variables
Backend (.env)
env
# Database
MYSQL_HOST=localhost
MYSQL_USER=app_user
MYSQL_PASSWORD=app_password
MYSQL_DB=personal_scheduler
MYSQL_PORT=3306

# Ollama
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=mistral:7b
OLLAMA_TIMEOUT=30

# App
SECRET_KEY=your-secret-key
DEBUG=True
Frontend (.env.local)
env
NEXT_PUBLIC_API_URL=http://localhost:5000
NEXT_PUBLIC_REQUEST_TIMEOUT=120000
TÃ¹y chá»n Model AI
Model	Size	Speed	Quality	Use Case
tinyllama	500MB	âš¡âš¡âš¡âš¡	â­â­	Demo, Testing
phi	1.6GB	âš¡âš¡âš¡	â­â­â­	General Purpose
mistral:7b	4.1GB	âš¡âš¡	â­â­â­â­	Production
codellama:7b	3.8GB	âš¡âš¡	â­â­â­â­	Text Processing
ğŸ—ƒï¸ Database Schema
sql
-- Users table
CREATE TABLE users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(100) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL
);

-- Schedules table  
CREATE TABLE schedules (
    id INT PRIMARY KEY AUTO_INCREMENT,
    user_id INT NOT NULL,
    title VARCHAR(255) NOT NULL,
    start_time DATETIME NOT NULL,
    end_time DATETIME NOT NULL,
    status ENUM('scheduled', 'completed', 'cancelled'),
    priority ENUM('low', 'medium', 'high')
);

ğŸ› ï¸ Development
Project Structure
text
ai_assistant_calendar_management/
â”œâ”€â”€ server/                 # Python Flask Backend
â”‚   â”œâ”€â”€ app.py             # Main application
â”‚   â”œâ”€â”€ models.py          # Database models
â”‚   â”œâ”€â”€ nlp_processor.py   # AI NLP processing
â”‚   â”œâ”€â”€ ai_assistant.py    # AI assistant logic
â”‚   â”œâ”€â”€ database.py        # Database management
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/              # Next.js Frontend
â”‚   â”œâ”€â”€ app/               # App router
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ lib/               # Utilities
â”‚   â”œâ”€â”€ hooks/             # Custom hooks
â”‚   â””â”€â”€ types/             # TypeScript types
â””â”€â”€ docker-compose.yml     # Docker configuration
API Endpoints
Method	Endpoint	Description
GET	/api/health	Health check
POST	/api/chat	Chat with AI assistant
GET	/api/schedules	Get user schedules
POST	/api/schedules	Create new schedule
GET	/api/schedules/upcoming	Get upcoming schedules
Development Commands
bash
# Backend development
cd server
python app.py

# Frontend development  
cd frontend
npm run dev

# Database management
docker exec -it mysqldb mysql -u app_user -p personal_scheduler

# Ollama management
docker exec -it ollama ollama list
ğŸ› Troubleshooting
Common Issues
Ollama connection timeout

bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama service
docker restart ollama
Database connection issues

bash
# Check MySQL container
docker compose logs mysql

# Reset database
docker compose down -v
docker compose up -d mysql
Model download issues

bash
# Clean pull
docker exec ollama ollama rm mistral:7b
docker exec ollama ollama pull mistral:7b
Performance Tips
Sá»­ dá»¥ng tinyllama cho development

TÄƒng RAM náº¿u model cháº­m

Sá»­ dá»¥ng SSD Ä‘á»ƒ tÄƒng tá»‘c model loading

Giá»›i háº¡n num_predict trong Ollama options

ğŸ¤ Contributing
We welcome contributions! Please see our Contributing Guide for details.

Development Setup
Fork the repository

Create a feature branch

Make your changes

Add tests

Submit a pull request

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
Ollama for making local AI accessible

Next.js for the amazing React framework

Flask for lightweight Python backend

Tailwind CSS for beautiful UI components

ğŸ“ Support
ğŸ“§ Email: support@example.com

ğŸ’¬ Issues: GitHub Issues

ğŸ“š Documentation: Project Wiki

